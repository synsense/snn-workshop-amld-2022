{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/synsense/snn-workshop-amld-2022/blob/master/2.%20vision%20processing/dance_recognition_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6f220d-c625-40d9-90cc-4bf94b5b51b0",
      "metadata": {
        "id": "8c6f220d-c625-40d9-90cc-4bf94b5b51b0"
      },
      "source": [
        "# Dance recognition challenge\n",
        "\n",
        "## Setup\n",
        "First we download the dataset and install missing packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oKkZE2mBvULV",
      "metadata": {
        "id": "oKkZE2mBvULV"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# - Download of dataset\n",
        "dataset_path = Path(\"/content\") / \"body-postures-dataset\"\n",
        "\n",
        "if not dataset_path.exists():\n",
        "  !wget -q --show-progress https://www.dropbox.com/s/b9gfafnh6aesrsu/body-postures-dataset.bin\n",
        "  # alternative mirror: https://lenzgregor.com/nextcloud/s/tbamCq9Eo95qfLc/download/body-postures-dataset.bin\n",
        "  !mv body-postures-dataset.bin body-postures-dataset.tar.gz\n",
        "  !tar -xzf body-postures-dataset.tar.gz\n",
        "\n",
        "# - Install dataset as package\n",
        "!pip install ./body-postures-dataset --quiet\n",
        "\n",
        "# - Install torchmetrics\n",
        "!pip install torchmetrics --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b430d407-1cf2-44d8-a40c-a0a18e178489",
      "metadata": {
        "id": "b430d407-1cf2-44d8-a40c-a0a18e178489"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from body_postures import BodyPostureFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data inspection\n",
        "\n",
        "The original data consits of a continuous stream of asynchronous events - each with its own time stamp. On an asynchronous neuromorhpic processor, the dynamics of the network are continuous in time and are directly linked to the timings of the input events.\n",
        "\n",
        "However, we are going to train our model on a CPU or GPU, which are synchronous (clocked) compute architectures. We therefore need to represent the envents in discretized time. We are going to use two different ways of arranging the events, depending on the type of neural network into which we are feeding the data.\n",
        "\n",
        "### Frame representation\n",
        "\n",
        "For artificial neural networks (ANNs), we will split the data stream of each recording into segments, each with the same, pre-defined number of events. \n",
        "\n",
        "In the individual segments we count the number of events for each pixel and channel. This way we eliminate the time dimension, such that each segment now has become a static frame. The frames could be compared to grey-scale images, with the brightness corresponding to the event count. Nothe that different to the frames of a conventional video camera, each frame here generally represents a different amount of time.\n",
        "\n",
        "The frame based representation is provided by `BodyPostureFrames`, which is a subclass of a PyTorch dataset."
      ],
      "metadata": {
        "id": "6WVpUOqzzib3"
      },
      "id": "6WVpUOqzzib3"
    },
    {
      "cell_type": "code",
      "source": [
        "?BodyPostureFrames"
      ],
      "metadata": {
        "id": "nhdPEU8-YTuD"
      },
      "id": "nhdPEU8-YTuD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use this class to provide the input for training an ANN. Let's generate a training and and a test set. It might take a few seconds to convert the events into frames."
      ],
      "metadata": {
        "id": "F8qJzAZG_7kT"
      },
      "id": "F8qJzAZG_7kT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4c86a1-e473-449a-b189-76a88a95769a",
      "metadata": {
        "id": "bb4c86a1-e473-449a-b189-76a88a95769a"
      },
      "outputs": [],
      "source": [
        "# The number of events per frame\n",
        "event_count = 3000\n",
        "\n",
        "# Ignore pixels where the firing frequency exceeds a given value\n",
        "hot_pixel_filter_freq = 60\n",
        "\n",
        "# Training dataset\n",
        "frame_dataset_training = BodyPostureFrames(\n",
        "    data_path=dataset_path / \"data\" / \"train\",\n",
        "    event_count=event_count,\n",
        "    hot_pixel_filter_freq=hot_pixel_filter_freq,\n",
        "    metadata_path=f'metadata/frames/train/{event_count}events',\n",
        ")\n",
        "\n",
        "# Validation dataset\n",
        "frame_dataset_validation = BodyPostureFrames(\n",
        "    data_path=dataset_path / \"data\" / \"test\",\n",
        "    event_count=event_count,\n",
        "    hot_pixel_filter_freq=hot_pixel_filter_freq,\n",
        "    metadata_path=f'metadata/frames/test/{event_count}events',\n",
        ")\n",
        "\n",
        "classes = frame_dataset_training.classes\n",
        "shape = frame_dataset_training.shape\n",
        "n_classes = len(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can happen that some pixels of our sensor just produce random events at a high rate. We want to ignore these. With the `hot_pixel_filter_freq` argument we set an upper limit for which event rate we consider reasonable. Any pixel emitting events at a higher rate is probably just generating noise and will be ignored. \n",
        "\n",
        "The `metadata_path` just stores some information about how the recordings are split into smaller sections in a file so that it doesn't have to be re-computed the next time.\n",
        "\n",
        "Let's have a closer look at our data! For this we pick some frame from our training set:"
      ],
      "metadata": {
        "id": "JJtpYK8fAgSE"
      },
      "id": "JJtpYK8fAgSE"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = 42\n",
        "frame, label = frame_dataset_training[sample_index]\n",
        "\n",
        "print(\"Frame data type:\", type(frame))\n",
        "print(\"Frame shape:\", frame.shape)\n",
        "print(\"Frame sum:\", frame.sum())\n",
        "print(\"Max. number events at single pixel:\", frame.max())\n",
        "plt.imshow(frame[0])"
      ],
      "metadata": {
        "id": "W_QtjT3DCBcB"
      },
      "id": "W_QtjT3DCBcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The frame is a numpy array of shape (1, 128, 128). The first dimension is the number of channels, which in this case is 1. The other two dimensions correspond to the image size. We see that our input has a resolution of 128x128.\n",
        "\n",
        "We also note that the total number of events in the frame is 3000, which is what we have previously defined with the `event_count` argument.\n",
        "\n",
        "When plotting the frame, brighter areas correspond to regions with a higher number of events. Because events are generated from intensity changes, those are the regions where something is happening. Any static background becomes invisible and we only see the shape of the person moving in front of the sensor."
      ],
      "metadata": {
        "id": "-_SGkLd7DEME"
      },
      "id": "-_SGkLd7DEME"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ground truth\n",
        "\n",
        "Every sample in the dataset comes with a label that enumerates the correct class. The `classes` dict maps the names of the classes to their integer labels. There are five poses (\"clap\", \"mj\", \"salive\", \"star\" and \"wave\") as well as a \"background\" class for recordings without any person and \"other\", when a person is not performing any known move."
      ],
      "metadata": {
        "id": "lTZugl6eGLq-"
      },
      "id": "lTZugl6eGLq-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G84vVQru6B_Z"
      },
      "outputs": [],
      "source": [
        "print(\"Label:\", label)\n",
        "print(classes)\n",
        "\n",
        "# Dict to map labels to classes\n",
        "lbl2clss = {lbl: clss for clss, lbl in classes.items()}"
      ],
      "id": "G84vVQru6B_Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View frames"
      ],
      "metadata": {
        "id": "TS_Xr2W1kUHb"
      },
      "id": "TS_Xr2W1kUHb"
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import rc\n",
        "\n",
        "rc('animation', html='jshtml')"
      ],
      "metadata": {
        "id": "LNHXu8r1kiH6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LNHXu8r1kiH6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following you can take a look at an animation of a short sequence of  frames. Change the `start_index` to see different parts of the dataset."
      ],
      "metadata": {
        "id": "191hI7v3kasq"
      },
      "id": "191hI7v3kasq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9013829-f2fd-4f0f-a8e3-c561f634064f"
      },
      "outputs": [],
      "source": [
        "start_index = 3050\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.tick_params(\n",
        "    axis=\"both\",\n",
        "    which=\"both\",\n",
        "    bottom=False,\n",
        "    left=False,\n",
        "    labelbottom=False,\n",
        "    labelleft=False\n",
        ")\n",
        "\n",
        "screen = ax.imshow(frame_dataset_training[start_index][0][0], vmin=0, vmax=10)\n",
        "\n",
        "def update_plot(idx):\n",
        "    frame, label = frame_dataset_training[start_index + idx]\n",
        "    screen.set_data(frame[0])\n",
        "    ax.set_title(f\"Label: {label} (`{lbl2clss[label]}`)\")\n",
        "    return screen, \n",
        "    \n",
        "anim = FuncAnimation(fig, update_plot, frames=20)\n",
        "plt.close()\n",
        "anim"
      ],
      "id": "b9013829-f2fd-4f0f-a8e3-c561f634064f"
    },
    {
      "cell_type": "markdown",
      "id": "25e34c81-5d62-41e6-bba8-c4ba432db4a0",
      "metadata": {
        "id": "25e34c81-5d62-41e6-bba8-c4ba432db4a0"
      },
      "source": [
        "### BONUS TASK: Data balancing\n",
        "\n",
        "The dataset is imbalanced. Let's look at training and testing set distribution and rectify the training set by using weighted random sampling.\n",
        "\n",
        "You can skip this task for now and come back to this task later. Just make sure to run the following cell to generate the data loader."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_loader = DataLoader(\n",
        "    dataset=frame_dataset_training, \n",
        "    batch_size=batch_size,\n",
        "    # sampler= ...\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=frame_dataset_validation,\n",
        "    batch_size=batch_size,\n",
        "    )"
      ],
      "metadata": {
        "id": "2EHJFbrQ00G4"
      },
      "id": "2EHJFbrQ00G4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample_dist(dataloader):\n",
        "  resampled_targets = []\n",
        "  for data, target in iter(dataloader):\n",
        "    resampled_targets.append(target)\n",
        "  resampled_targets = torch.cat(resampled_targets).numpy()\n",
        "\n",
        "  plt.bar(classes.keys(), np.bincount(resampled_targets))\n",
        "  plt.ylabel(\"Number of samples\")"
      ],
      "metadata": {
        "id": "EJ3jS12g_Uvp"
      },
      "id": "EJ3jS12g_Uvp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample_dist(train_loader)"
      ],
      "metadata": {
        "id": "0F5HnnSuAjZZ"
      },
      "id": "0F5HnnSuAjZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample_dist(test_loader)"
      ],
      "metadata": {
        "id": "TkFhBEcR_icT"
      },
      "id": "TkFhBEcR_icT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a5e91720-80f1-4126-8593-4630c5b7b5d3",
      "metadata": {
        "id": "a5e91720-80f1-4126-8593-4630c5b7b5d3"
      },
      "source": [
        "### Settings and hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try out different learning rates and weight decays here, but the provided defaults should yield reasonable results."
      ],
      "metadata": {
        "id": "SypRWbGqF6FF"
      },
      "id": "SypRWbGqF6FF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5f2504-9edf-42ff-aca6-ec5018bbd53b",
      "metadata": {
        "id": "af5f2504-9edf-42ff-aca6-ec5018bbd53b"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "weight_decay = 0\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device {device}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e251d3-a334-4a84-85d2-e4cafa79e8ec",
      "metadata": {
        "id": "31e251d3-a334-4a84-85d2-e4cafa79e8ec"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suggest to use the following model architecture. Remember, the input shape of each frame is (1, 128, 128). Note that we disable biases because they are tricky to convert to SNNs and in this case not needed."
      ],
      "metadata": {
        "id": "BKUH2XrsGX1c"
      },
      "id": "BKUH2XrsGX1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa05cdb9-08ac-4cab-b3d0-0c9c2d705b73",
      "metadata": {
        "id": "fa05cdb9-08ac-4cab-b3d0-0c9c2d705b73"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Sequential):\n",
        "    def __init__(self, n_classes=10):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(1, 16, kernel_size=(3, 3), stride=2, padding=1, bias=False),  # 16, 64, 64\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(2, 2)),  # 16, 32, 32\n",
        "            nn.Dropout2d(0.1),\n",
        "            \n",
        "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=2, padding=1, bias=False),  # 32, 16, 16\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(2, 2)),  # 32, 8, 8\n",
        "            nn.Dropout2d(0.25),\n",
        "            \n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1, bias=False),  # 64, 4, 4\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(2, 2)),  # 64, 2, 2\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            \n",
        "            nn.Linear(32*4*4, n_classes, bias=False),\n",
        "        )\n",
        "\n",
        "ann = ANN()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "The stage is yours! Use the provided data loaders and model architecture to train an ANN that is able to classify the dance poses."
      ],
      "metadata": {
        "id": "oyUYhZGAJZNH"
      },
      "id": "oyUYhZGAJZNH"
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Train the ANN here using the provided train_loader, test_loader and model.\n",
        "\n"
      ],
      "metadata": {
        "id": "WMRAxsTyJvZn"
      },
      "id": "WMRAxsTyJvZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert ANN to SNN"
      ],
      "metadata": {
        "id": "LWRzKLNyMpLr"
      },
      "id": "LWRzKLNyMpLr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16f5cdd2-13a5-4cfc-80a0-dc4b9c2d7369"
      },
      "source": [
        "For working with SNNs we will use our in-house library Sinabs. You can find out more about it here: https://sinabs.readthedocs.io . You should not need to change any cell contents from here onwards as long as you provide the trained ann."
      ],
      "id": "16f5cdd2-13a5-4cfc-80a0-dc4b9c2d7369"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quHtcngg6V17"
      },
      "outputs": [],
      "source": [
        "!pip install sinabs --quiet"
      ],
      "id": "quHtcngg6V17"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before converting our trained ANN to a spiking neural network, we need to make sure that the weights are scaled correctly, such that the spiking activities in each layer are normalized. For this we use the `normalize_weights` function of sinabs:"
      ],
      "metadata": {
        "id": "ShcL0JIgrdBt"
      },
      "id": "ShcL0JIgrdBt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tiOrn7G6V18"
      },
      "outputs": [],
      "source": [
        "import sinabs\n",
        "import copy\n",
        "\n",
        "# - Generate a copy with original weights\n",
        "ann_original = copy.deepcopy(ann)\n",
        "\n",
        "# - Find names of activation layers, that will be converted to spiking layers\n",
        "spike_layers = [name for name, child in ann.cpu().named_children() if isinstance(child, nn.ReLU)]\n",
        "# - Find names of layers with weights to be scaled\n",
        "param_layers = [name for name, child in ann.cpu().named_children() if isinstance(child, (nn.Conv2d, nn.Linear))]\n",
        "# - First data sample from the training set to gauge network activity\n",
        "data = next(iter(train_loader))[0]\n",
        "\n",
        "# - Normalize the weights of the ANN\n",
        "sinabs.utils.normalize_weights(ann, data, output_layers=spike_layers, param_layers=param_layers)"
      ],
      "id": "8tiOrn7G6V18"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLLtR13E6V18"
      },
      "source": [
        "We can scale up the weights of the first layer. This increases the overall activity in our network, such that it becomes a closer approximation of the ReLU activations in the ANN. Don't exaggerate, however. Too strong activity in the network will drive up power consumption on the hardware."
      ],
      "id": "nLLtR13E6V18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0FiAwF76V18"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  ann[0].weight *= 5"
      ],
      "id": "w0FiAwF76V18"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our ANN is ready to be converted to an SNN. For this, we need to provide the shape of the input. We also tell sinabs to add another spiking layer after the final layer of our network. This is to make sure that the SNN output consists of events, for compatibility with our Speck chip."
      ],
      "metadata": {
        "id": "FmNBetUNtIyJ"
      },
      "id": "FmNBetUNtIyJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSXns8uL6V18"
      },
      "outputs": [],
      "source": [
        "snn = sinabs.from_torch.from_model(ann, input_shape=(1, 128, 128), add_spiking_output=True)\n",
        "snn"
      ],
      "id": "qSXns8uL6V18"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the new SNN, we see that it contains a copy of the original ANN as well as the actual spiking neural network. In the forward call, only the spiking part will be active. "
      ],
      "metadata": {
        "id": "iuqAH1E1uPVG"
      },
      "id": "iuqAH1E1uPVG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv8tqKT76V19"
      },
      "outputs": [],
      "source": [
        "# - Save the model\n",
        "\n",
        "torch.save(snn, 'spiking_model.pth')"
      ],
      "id": "Cv8tqKT76V19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ns2PMot6V19"
      },
      "outputs": [],
      "source": [
        "# # Load previously saved model\n",
        "\n",
        "# snn = torch.load('spiking_model.pth')\n",
        "# print(\"Loaded saved model.\")"
      ],
      "id": "3ns2PMot6V19"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvMRaqba6V19"
      },
      "source": [
        "## Evaluate converted SNN on event data\n",
        "\n",
        "Before testing our SNN, let's have a brief look at the data we are feeding it.\n",
        "\n",
        "### Event Representation\n",
        "\n",
        "For spiking neural networks (SNNs) we use a similar representation to the frames we used for the ANN. However, here we want to maintain a notion of time, to mimick the neuron dynamics on the neuromorphic processor. We therefore arrange our events in time along a grid of very short intervals, corresponding to a simulation time step. This time step should be chosen small enough such that the neuron dynamics are well approximated. On the other hand, choosing it too short will slow down our simulations.\n",
        "\n",
        "Data in this format will be porovided to us by the class `BodyPostureEvents`."
      ],
      "id": "XvMRaqba6V19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH1wVjNy6V19"
      },
      "outputs": [],
      "source": [
        "from body_postures import BodyPostureEvents"
      ],
      "id": "yH1wVjNy6V19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FGxuiNh6V19"
      },
      "outputs": [],
      "source": [
        "?BodyPostureEvents"
      ],
      "id": "0FGxuiNh6V19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5jrPkvC6V19"
      },
      "outputs": [],
      "source": [
        "slice_dt = 2e6 # microseconds\n",
        "bin_dt = 2e3 # microseconds\n",
        "batch_size = 1\n",
        "\n",
        "raster_test_dataset = BodyPostureEvents(\n",
        "    data_path=dataset_path / \"data\" / \"test\",\n",
        "    cache_path=f\"cache/test/{slice_dt}/{bin_dt}\",\n",
        "    slice_dt=slice_dt,\n",
        "    bin_dt=bin_dt,\n",
        "    metadata_path=f\"metadata/raster/test/{slice_dt}/{bin_dt}\",\n",
        "    hot_pixel_filter_freq=hot_pixel_filter_freq,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(raster_test_dataset, shuffle=True, batch_size=batch_size)"
      ],
      "id": "h5jrPkvC6V19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data now. You can set the `sample_index` to inspect different samples from the dataset. In contrast to the frames of the `BodyPostureFrames` data, the events of each sample are spread along a time axis. Therefore we see way fewer events at any given time bin. Note that due to computational limiations we are only animating the first few timesteps of each sample."
      ],
      "metadata": {
        "id": "TyimxGeHmuDQ"
      },
      "id": "TyimxGeHmuDQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFM87CoNll3j"
      },
      "outputs": [],
      "source": [
        "sample_index = 0\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.tick_params(\n",
        "    axis=\"both\",\n",
        "    which=\"both\",\n",
        "    bottom=False,\n",
        "    left=False,\n",
        "    labelbottom=False,\n",
        "    labelleft=False\n",
        ")\n",
        "\n",
        "screen = ax.imshow(raster_test_dataset[sample_index][0][0, 0], vmin=0, vmax=1)\n",
        "\n",
        "def update_plot(idx):\n",
        "    frame, label = raster_test_dataset[sample_index]\n",
        "    screen.set_data(frame[idx, 0])\n",
        "    ax.set_title(f\"Label: {label} (`{lbl2clss[label]}`)\")\n",
        "    return screen, \n",
        "    \n",
        "anim = FuncAnimation(fig, update_plot, frames=20)\n",
        "plt.close()\n",
        "anim"
      ],
      "id": "iFM87CoNll3j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on the SNN\n",
        "\n",
        "Time to feed our SNN with data and see how it performs!"
      ],
      "metadata": {
        "id": "wqx0KetIw7oD"
      },
      "id": "wqx0KetIw7oD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2da1idK4ekvE"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "snn.eval()\n",
        "snn = snn.to(device)\n",
        "\n",
        "val_data = []\n",
        "pbar =  tqdm(test_loader)\n",
        "for data, labels in pbar:\n",
        "  with torch.no_grad():\n",
        "    snn.reset_states()\n",
        "    data = data.to(device)\n",
        "    data = data.flatten(0, 1).float()\n",
        "    spikes = snn(data).unflatten(0, (batch_size, -1))\n",
        "\n",
        "    # Collect outputs and labels\n",
        "    predictions = spikes.sum(1).argmax(dim=1) \n",
        "    val_data.append((predictions.detach().cpu(), labels.detach().cpu()))\n",
        "\n"
      ],
      "id": "2da1idK4ekvE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=n_classes)\n",
        "confusion_list = [confusion_matrix(pred, label) for pred, label in val_data]\n",
        "confusion = torch.stack(confusion_list).sum(0)\n",
        "val_accuracy = (confusion.trace() / confusion.sum()).item()\n",
        "\n",
        "print(f\"Validation accuracy on SNN: {val_accuracy}\")"
      ],
      "metadata": {
        "id": "_Impnh8BWGb4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_Impnh8BWGb4"
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = 0.5\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "plot_confusion_matrix(confusion.numpy(), classes.keys())"
      ],
      "metadata": {
        "id": "tFNj-VRYOiIn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tFNj-VRYOiIn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run your model on neuromorphic hardware\n",
        "\n",
        "Upload your model to the following dropbox link: https://www.dropbox.com/request/uwBr6x2kkuV9hC15xNSX\n",
        "and talk to one of the organizers to see your model in action on our Speck chip!"
      ],
      "metadata": {
        "id": "nxArv3B1pdYZ"
      },
      "id": "nxArv3B1pdYZ"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5193LM6bRAJT"
      },
      "id": "5193LM6bRAJT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of Copy of AMLD_train_full.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}